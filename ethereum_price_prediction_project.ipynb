{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Ethereum Price Prediction with ARIMA, LSTM and Random Forest\n", "\n", "This notebook implements your self project:\n", "\n", "- **Objective:** Build a scalable predictive modeling framework to forecast Ethereum closing prices.\n", "- **Data:** Daily ETH-USD prices (2500+ records).\n", "- **Models:** ARIMA, LSTM, Decision Tree and Random Forest.\n", "- **Metrics:** RMSE and MAPE to compare short-term and long-term forecasting performance.\n", "- **Outcome:** A reusable 30-day forecasting pipeline and visualizations for ETH trading insights.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Setup\n", "\n", "Run the cell below if you are on a fresh environment (e.g. Google Colab). Comment it out on your local machine if you already have the libraries installed."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# !pip install yfinance statsmodels scikit-learn tensorflow matplotlib pandas numpy"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error\n", "from sklearn.metrics import r2_score\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.preprocessing import MinMaxScaler\n", "\n", "import yfinance as yf\n", "from statsmodels.tsa.arima.model import ARIMA\n", "\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import LSTM, Dense\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "\n", "plt.rcParams['figure.figsize'] = (10, 4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Helper functions\n", "\n", "We define evaluation metrics, a function to create supervised sequences from time series data, and some plotting utilities."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def mape(y_true, y_pred):\n", "    \"\"\"Mean Absolute Percentage Error (in %).\n", "    Adds a small epsilon to avoid division by zero.\n", "    \"\"\"\n", "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n", "    eps = 1e-8\n", "    return np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100\n", "\n", "\n", "def evaluate_regression(y_true, y_pred, name=\"model\"):\n", "    mse = mean_squared_error(y_true, y_pred)\n", "    rmse = np.sqrt(mse)\n", "    mae = mean_absolute_error(y_true, y_pred)\n", "    mape_val = mape(y_true, y_pred)\n", "    r2 = r2_score(y_true, y_pred)\n", "    print(f\"{name} -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, MAPE: {mape_val:.2f}%, R2: {r2:.4f}\")\n", "    return {\"model\": name, \"rmse\": rmse, \"mae\": mae, \"mape\": mape_val, \"r2\": r2}\n", "\n", "\n", "def create_sequences(series, window_size=60):\n", "    \"\"\"Create (X, y) pairs using a rolling window over a 1D array or 2D array.\n", "    series is expected to be a 2D array of shape (n_samples, n_features).\n", "    \"\"\"\n", "    X, y = [], []\n", "    for i in range(len(series) - window_size):\n", "        X.append(series[i : i + window_size])\n", "        y.append(series[i + window_size])\n", "    return np.array(X), np.array(y)\n", "\n", "\n", "def plot_predictions(dates, y_true, y_pred, title):\n", "    plt.figure()\n", "    plt.plot(dates, y_true, label=\"True\")\n", "    plt.plot(dates, y_pred, label=\"Predicted\")\n", "    plt.title(title)\n", "    plt.xlabel(\"Date\")\n", "    plt.ylabel(\"ETH Close Price (USD)\")\n", "    plt.legend()\n", "    plt.tight_layout()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Data Collection & Preprocessing\n", "\n", "- We use **yfinance** to download daily ETH-USD closing prices.\n", "- Time range is chosen to have 2500+ observations.\n", "- We keep the `Close` column and handle missing values if any."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["symbol = \"ETH-USD\"\n", "start_date = \"2015-01-01\"  # early enough to get 2500+ daily records\n", "\n", "eth_df = yf.download(symbol, start=start_date)\n", "eth_df = eth_df.sort_index()\n", "\n", "# Keep only the closing price\n", "eth_df = eth_df[[\"Close\"]].dropna()\n", "\n", "print(\"Number of daily records:\", len(eth_df))\n", "eth_df.head()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["eth_df.tail()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "plt.plot(eth_df.index, eth_df[\"Close\"].values)\n", "plt.title(\"ETH-USD Closing Price\")\n", "plt.xlabel(\"Date\")\n", "plt.ylabel(\"Price (USD)\")\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Basic Feature Engineering\n", "\n", "For tree-based models we can include simple technical features:\n", "- 1-day return\n", "- 7-day and 30-day moving averages\n", "- 7-day rolling volatility\n", "\n", "The LSTM and ARIMA models will use the raw closing price series (scaled / transformed appropriately)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["features_df = eth_df.copy()\n", "features_df[\"return_1d\"] = features_df[\"Close\"].pct_change()\n", "features_df[\"ma_7\"] = features_df[\"Close\"].rolling(window=7).mean()\n", "features_df[\"ma_30\"] = features_df[\"Close\"].rolling(window=30).mean()\n", "features_df[\"vol_7\"] = features_df[\"return_1d\"].rolling(window=7).std()\n", "\n", "features_df = features_df.dropna()\n", "features_df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Train / Test Split\n", "\n", "We use the first 80% of the time series for training and the remaining 20% for testing.\n", "\n", "- **ARIMA** operates on the univariate closing price.\n", "- **LSTM / Decision Tree / Random Forest** use a sliding window over the scaled closing price.\n", "- The same test set is used for all models to make metrics comparable."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# For ARIMA (univariate series)\n", "train_size_arima = int(len(eth_df) * 0.8)\n", "train_arima = eth_df[\"Close\"].iloc[:train_size_arima]\n", "test_arima = eth_df[\"Close\"].iloc[train_size_arima:]\n", "\n", "print(\"ARIMA train size:\", len(train_arima), \"test size:\", len(test_arima))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# For LSTM / Tree models, use scaled close price and create sequences\n", "values = eth_df[\"Close\"].values.reshape(-1, 1)\n", "scaler = MinMaxScaler()\n", "values_scaled = scaler.fit_transform(values)\n", "\n", "window_size = 60\n", "X_seq, y_seq = create_sequences(values_scaled, window_size=window_size)\n", "\n", "train_size_seq = int(len(X_seq) * 0.8)\n", "\n", "X_train_seq = X_seq[:train_size_seq]\n", "y_train_seq = y_seq[:train_size_seq]\n", "X_test_seq = X_seq[train_size_seq:]\n", "y_test_seq = y_seq[train_size_seq:]\n", "\n", "dates_all = eth_df.index[window_size:]\n", "train_dates_seq = dates_all[:train_size_seq]\n", "test_dates_seq = dates_all[train_size_seq:]\n", "\n", "print(\"Sequence train shape:\", X_train_seq.shape, \"test shape:\", X_test_seq.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. ARIMA Model (Classical Time Series)\n", "\n", "We fit an ARIMA model on the training closing prices and forecast over the test horizon. The order (p,d,q) is chosen manually here; in a more advanced setting you can use auto-ARIMA to tune this."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["p, d, q = 5, 1, 0  # basic order; can be tuned further\n", "\n", "arima_model = ARIMA(train_arima, order=(p, d, q))\n", "arima_result = arima_model.fit()\n", "print(arima_result.summary())"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["arima_forecast = arima_result.forecast(steps=len(test_arima))\n", "arima_forecast.index = test_arima.index\n", "\n", "metrics_arima = evaluate_regression(test_arima.values, arima_forecast.values, name=\"ARIMA\")\n", "plot_predictions(test_arima.index, test_arima.values, arima_forecast.values, \"ARIMA - Test Forecast\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Decision Tree & Random Forest (Tree-based Models)\n", "\n", "Here we treat the problem as supervised regression with lag features:\n", "\n", "- Inputs: last 60 days of (scaled) closing prices.\n", "- Target: next day closing price.\n", "\n", "We train a **Decision Tree Regressor** as a simple baseline and a **Random Forest Regressor** as an ensemble improvement."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Flatten time dimension into feature vectors\n", "X_train_flat = X_train_seq.reshape(X_train_seq.shape[0], -1)\n", "X_test_flat = X_test_seq.reshape(X_test_seq.shape[0], -1)\n", "\n", "# Decision Tree\n", "dt = DecisionTreeRegressor(max_depth=10, random_state=42)\n", "dt.fit(X_train_flat, y_train_seq.ravel())\n", "\n", "dt_pred_scaled = dt.predict(X_test_flat).reshape(-1, 1)\n", "dt_pred = scaler.inverse_transform(dt_pred_scaled).ravel()\n", "y_test_dt = scaler.inverse_transform(y_test_seq.reshape(-1, 1)).ravel()\n", "\n", "metrics_dt = evaluate_regression(y_test_dt, dt_pred, name=\"Decision Tree\")\n", "plot_predictions(test_dates_seq, y_test_dt, dt_pred, \"Decision Tree - Test Predictions\")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Random Forest\n", "rf = RandomForestRegressor(\n", "    n_estimators=300,\n", "    max_depth=None,\n", "    random_state=42,\n", "    n_jobs=-1\n", ")\n", "rf.fit(X_train_flat, y_train_seq.ravel())\n", "\n", "rf_pred_scaled = rf.predict(X_test_flat).reshape(-1, 1)\n", "rf_pred = scaler.inverse_transform(rf_pred_scaled).ravel()\n", "y_test_rf = scaler.inverse_transform(y_test_seq.reshape(-1, 1)).ravel()\n", "\n", "metrics_rf = evaluate_regression(y_test_rf, rf_pred, name=\"Random Forest\")\n", "plot_predictions(test_dates_seq, y_test_rf, rf_pred, \"Random Forest - Test Predictions\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. LSTM Model (Deep Learning for Time Series)\n", "\n", "An LSTM network can capture temporal dependencies and nonlinear patterns in the ETH price series. We use:\n", "\n", "- One LSTM layer with 50 units\n", "- Dense output layer\n", "- Early stopping on validation loss to avoid overfitting"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X_train_lstm = X_train_seq\n", "X_test_lstm = X_test_seq\n", "\n", "model_lstm = Sequential([\n", "    LSTM(50, input_shape=(window_size, 1)),\n", "    Dense(1)\n", "])\n", "\n", "model_lstm.compile(optimizer=\"adam\", loss=\"mse\")\n", "\n", "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n", "\n", "history = model_lstm.fit(\n", "    X_train_lstm,\n", "    y_train_seq,\n", "    epochs=50,\n", "    batch_size=32,\n", "    validation_split=0.2,\n", "    callbacks=[early_stop],\n", "    verbose=1\n", ")"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["lstm_pred_scaled = model_lstm.predict(X_test_lstm)\n", "y_test_lstm_scaled = y_test_seq.reshape(-1, 1)\n", "\n", "lstm_pred = scaler.inverse_transform(lstm_pred_scaled).ravel()\n", "y_test_lstm = scaler.inverse_transform(y_test_lstm_scaled).ravel()\n", "\n", "metrics_lstm = evaluate_regression(y_test_lstm, lstm_pred, name=\"LSTM\")\n", "plot_predictions(test_dates_seq, y_test_lstm, lstm_pred, \"LSTM - Test Predictions\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### LSTM vs Decision Tree: Relative Improvement\n", "\n", "To match your resume bullet, we can compute the relative improvement of LSTM over the Decision Tree in terms of MAPE. The exact percentage will depend on the run and data window, but typically LSTM should give a lower error on this task."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["improvement_mape = (metrics_dt[\"mape\"] - metrics_lstm[\"mape\"]) / metrics_dt[\"mape\"] * 100\n", "print(f\"LSTM MAPE improvement over Decision Tree: {improvement_mape:.2f}%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 7. 30-Day Forecasting Pipeline (Long-Term Horizon)\n", "\n", "We now build a simple **30-day ahead** forecasting pipeline using the trained LSTM model:\n", "\n", "- Use the last 60 days of scaled ETH prices as the initial window.\n", "- Iteratively forecast the next day, append the prediction, and slide the window.\n", "- Invert the scaling to get price forecasts in USD.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def multi_step_forecast_lstm(model, last_window_scaled, scaler, n_steps=30):\n", "    \"\"\"Iteratively forecast n_steps into the future using an LSTM model.\n", "    last_window_scaled: array of shape (window_size, 1) in scaled space.\n", "    Returns: (n_steps,) array in original price space.\n", "    \"\"\"\n", "    window = last_window_scaled.copy()\n", "    preds_scaled = []\n", "    for _ in range(n_steps):\n", "        x_input = window.reshape(1, window.shape[0], window.shape[1])\n", "        yhat_scaled = model.predict(x_input, verbose=0)\n", "        preds_scaled.append(yhat_scaled[0, 0])\n", "        window = np.vstack([window[1:], yhat_scaled])\n", "    preds_scaled = np.array(preds_scaled).reshape(-1, 1)\n", "    preds = scaler.inverse_transform(preds_scaled).ravel()\n", "    return preds\n", "\n", "# Use last window_size days from the full series for forecasting\n", "last_window_scaled = values_scaled[-window_size:]\n", "n_future_days = 30\n", "future_preds = multi_step_forecast_lstm(model_lstm, last_window_scaled, scaler, n_steps=n_future_days)\n", "\n", "last_date = eth_df.index[-1]\n", "future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=n_future_days, freq=\"D\")\n", "\n", "plt.figure()\n", "plt.plot(eth_df.index[-120:], eth_df[\"Close\"].values[-120:], label=\"History (last 120 days)\")\n", "plt.plot(future_dates, future_preds, label=\"LSTM 30-day Forecast\")\n", "plt.title(\"ETH-USD 30-Day Ahead Forecast (LSTM)\")\n", "plt.xlabel(\"Date\")\n", "plt.ylabel(\"Price (USD)\")\n", "plt.legend()\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Model Comparison Summary\n", "\n", "Finally, we gather all the metrics into a single table to compare ARIMA, Decision Tree, Random Forest and LSTM on the same test period."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["summary_df = pd.DataFrame([\n", "    metrics_arima,\n", "    metrics_dt,\n", "    metrics_rf,\n", "    metrics_lstm\n", "])\n", "summary_df.set_index(\"model\", inplace=True)\n", "summary_df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### How this notebook maps to your resume bullets\n", "\n", "- **Collected and preprocessed Ethereum historical data with 2,500+ daily records**  \n", "  \u2192 Data download, cleaning and feature engineering sections.\n", "- **Trained & optimized ARIMA, LSTM and Random Forest predictive models**  \n", "  \u2192 ARIMA, LSTM, Decision Tree and Random Forest sections with train/test splits and early stopping.\n", "- **Evaluated models using RMSE and MAPE**  \n", "  \u2192 `evaluate_regression` function and `summary_df` table.\n", "- **Captured temporal dependencies with LSTM, reducing prediction error over Tree models**  \n", "  \u2192 LSTM vs Decision Tree comparison and improvement calculation.\n", "- **Developed a reliable 30-day forecasting pipeline**  \n", "  \u2192 `multi_step_forecast_lstm` function and 30-day forecast plot.\n", "\n", "You can now upload this notebook to your GitHub repository named something like `ethereum-price-prediction` and reference it directly from your resume."]}]}